{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from atod import Heroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No abilities for this HeroID == 16\n"
     ]
    }
   ],
   "source": [
    "heroes = Heroes.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_many(string: str, replacements: list):\n",
    "    ''' Performs many str.replace() functions in a row.\n",
    "    \n",
    "    Args:\n",
    "        string: string to be changed\n",
    "        replacements (list of tuples): tuples are args for replace function\n",
    "            in form (old, new)\n",
    "            \n",
    "    Returns:\n",
    "        str: `string` on which all replace() functions were performed\n",
    "        \n",
    "    '''\n",
    "    for repl in replacements:\n",
    "        string = string.replace(*repl)\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    ''' Returns amount of unique words in the document.'''\n",
    "    words = set()\n",
    "    for text in doc:\n",
    "        words = words.union([w for w in text.split()])\n",
    "        \n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Abilities' object has no attribute 'get_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-839ac160e129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m abilities = {h.name: list(map(lambda x: replace_many(x, replacements), \n\u001b[1;32m      5\u001b[0m                               h.abilities.get_description(['texts'])))\n\u001b[0;32m----> 6\u001b[0;31m                      for h in heroes}\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtexts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhero\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhero\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-839ac160e129>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m abilities = {h.name: list(map(lambda x: replace_many(x, replacements), \n\u001b[1;32m      5\u001b[0m                               h.abilities.get_description(['texts'])))\n\u001b[0;32m----> 6\u001b[0;31m                      for h in heroes}\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtexts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhero\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhero\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Abilities' object has no attribute 'get_description'"
     ]
    }
   ],
   "source": [
    "# replace (old, new)\n",
    "replacements = [('%', ''), ('\\\\n', ' '), ('%%', ''), ('_', ' ')]\n",
    "\n",
    "abilities = {h.name: list(map(lambda x: replace_many(x, replacements), \n",
    "                              h.abilities.get_description(['texts'])))\n",
    "                     for h in heroes}\n",
    "\n",
    "texts_list = [text for hero in abilities.values() for text in hero]\n",
    "print(count_words(texts_list))\n",
    "\n",
    "# What above dict comprehension does: \n",
    "# abilities = dict()\n",
    "# for hero in heroes:\n",
    "#     abilities[hero.name] = list()\n",
    "#     for ability in hero.abilities.get_texts():\n",
    "#         abilities[hero.name].append(ability.replace('\\\\n', ' ').replace('%%', '%'))\n",
    "# I just love lambdas and comprehensions, so :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of stop words \n",
    "# names of heroes commonly occur in descriptions, so\n",
    "# there is need to remove them\n",
    "heroes_names = [h.name for h in heroes]\n",
    "words_in_heroes_names = [word.lower() \n",
    "                         for name in heroes_names \n",
    "                         for word in name.split(' ')]\n",
    "\n",
    "eng_stop_words = TfidfVectorizer(stop_words='english').get_stop_words()\n",
    "stop_words = set(words_in_heroes_names + list(eng_stop_words) \n",
    "                 + ['font', 'color', '7998b5', 'target', 'enemy', 'friendly', 'allied',\n",
    "                    'remnant', 'aghanim', 'scepter', 'units', 'deal damage'\n",
    "                    'cause', 'creep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stemmer = EnglishStemmer()\n",
    "corpus = dict()\n",
    "\n",
    "# # for every hero\n",
    "for hero, texts in abilities.items():    \n",
    "#     stemmed_corpus[hero] = list()\n",
    "#     # concatenate all abilities descriptions into one\n",
    "#     # and stem all words inside\n",
    "    corpus[hero] = ' '.join([word for doc in texts\n",
    "                                  for word in doc.split(' ')])\n",
    "    \n",
    "# print(count_words(list(stemmed_corpus.values())))\n",
    "    \n",
    "# stemmed_stop_words = [stemmer.stem(word) for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             min_df=2,)\n",
    "vectorizer.fit(corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the most popular words\n",
    "most_popular_words = [('', 0)] * 20\n",
    "print(most_popular_words)\n",
    "\n",
    "id2word = {str(id_): word for word, id_ in vectorizer.vocabulary_.items()}\n",
    "corpus_matrix = vectorizer.transform(corpus.values())\n",
    "\n",
    "for index in range(corpus_matrix.shape[1]):\n",
    "    col = corpus_matrix.getcol(index)\n",
    "    \n",
    "    if col.nnz > most_popular_words[0][1]:\n",
    "        most_popular_words[0] = (id2word[str(index)], \n",
    "                                  col.nnz)\n",
    "        most_popular_words = sorted(most_popular_words, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(most_popular_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "TFIDF doesn't work for this task, because meaningful words ('stun', 'silence') are common in the corpus and in the single texts they occur same amount of times with unimportant words.\n",
    "\n",
    "CountVectorizer doesn't work because important word can occur in the text the same amount of times as unimportant one. To improve this method, one can weight words based on how many times they occur in the corpus.\n",
    "\n",
    "There are too few documents to try something like word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
